{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sim = msp.simulate(sample_size=2, length=L, mutation_rate=1e-8, recombination_rate=1e-9, Ne=1e4)\n",
    "seq = np.zeros(L, dtype='u1')\n",
    "positions = np.array([v.position for v in sim.variants()])\n",
    "seq[positions.astype(int)] = 1\n",
    "gaps = np.diff(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime as msp\n",
    "L = 1_000_000\n",
    "sim = msp.simulate(sample_size=2, length=L, mutation_rate=1e-8, recombination_rate=1e-9, Ne=1e4)\n",
    "\n",
    "import numpy as np\n",
    "x = []\n",
    "y = []\n",
    "i = 0\n",
    "while i < 10007:\n",
    "    L = 1_000_000\n",
    "    sim = msp.simulate(sample_size=2, length=L, mutation_rate=1e-8, recombination_rate=1e-9, Ne=1e4)\n",
    "    positions = np.array([v.position for v in sim.variants()])\n",
    "    gaps = np.diff(positions)\n",
    "    if len(gaps) <= 100:\n",
    "        continue\n",
    "    x.append(gaps[:50])\n",
    "    y.append(gaps[50:100])\n",
    "    i += 1\n",
    "    \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1132)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get one long sequence \n",
    "import numpy as np\n",
    "i = 0\n",
    "corpus = []\n",
    "while i < 5000:\n",
    "    L = 1_000_000\n",
    "    sim = msp.simulate(sample_size=2, length=L, mutation_rate=1e-8, recombination_rate=1e-9, Ne=1e4)\n",
    "    positions = np.array([v.position for v in sim.variants()])\n",
    "    gaps = np.diff(positions)\n",
    "    if len(gaps) > 1000:\n",
    "        corpus.append(gaps)\n",
    "    i += 1\n",
    "corpus=np.array(corpus)\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/Users/peng/Desktop/Gene_research/Genseq.txt\", corpus,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (5000, 50)\n",
      "y shape: (5000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"x shape:\",x.shape)\n",
    "print(\"y shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten x to compute number of unique numbers\n",
    "        \n",
    "def top_num(x,y):\n",
    "    flattened = []\n",
    "    for sublist in x:\n",
    "        for val in sublist:\n",
    "            flattened.append(val)\n",
    "\n",
    "    flattened_y = []\n",
    "    for sublist in y:\n",
    "        for val in sublist:\n",
    "            flattened_y.append(val)\n",
    "    \n",
    "    top = 0\n",
    "    top = max(max(flattened), max(flattened_y))\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value in x: 370282.9671637656\n",
      "max value in y: 410573.1911046213\n",
      "410573.1911046213\n"
     ]
    }
   ],
   "source": [
    "print(\"max value in x:\",max(flattened))\n",
    "print(\"max value in y:\",max(flattened_y))\n",
    "print(top_num(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 440.98916468,  708.85193645,  491.03451468,  162.67687766,\n",
       "       3635.91610171, 1898.79648583,  745.71080997,  282.66494379,\n",
       "       1351.07065688,  316.17803887,  426.11580737,  323.67358783,\n",
       "        523.90315051,  266.71941932, 1098.65226744, 2809.43993762,\n",
       "        476.86280551, 1141.21526702,  515.23499611,  400.54924577,\n",
       "        156.87796986, 1473.974412  ,   83.80657142,  562.86770954,\n",
       "       1030.53364366,  163.94432049,   76.60956439, 2370.97711162,\n",
       "        868.36684356, 3043.22318916,  120.51624077, 1347.20778079,\n",
       "       2618.26785486,  447.59704267, 3057.49840218, 3230.37790138,\n",
       "        777.5921285 ,  616.24722602, 2881.7761304 , 1582.97307948,\n",
       "       2263.5372434 ,  884.05845368,  950.67238051,  391.19517475,\n",
       "       1360.20287021,  519.50072365, 2291.29195493,  785.92775552,\n",
       "        932.24776841,  495.16402436])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.36113512e+03, 2.08596703e+02, 1.32026640e+03, 2.05246535e+03,\n",
       "       1.64380977e+02, 1.52989251e+03, 2.53147235e+02, 1.61062328e+03,\n",
       "       4.38016586e+00, 1.69247734e+03, 8.30554959e+03, 9.67658820e+02,\n",
       "       1.57163517e+03, 4.35037946e+03, 3.99516569e+03, 1.43049889e+03,\n",
       "       8.55061849e+02, 1.31958156e+02, 1.70789168e+03, 3.05887263e+02,\n",
       "       2.61561900e+02, 6.29696603e+03, 1.49203400e+02, 1.46946394e+03,\n",
       "       2.60264320e+03, 1.53776930e+03, 8.39698119e+02, 3.38859204e+03,\n",
       "       3.35407749e+03, 7.60133974e+02, 1.76788313e+03, 1.68681423e+03,\n",
       "       7.60025268e+02, 2.58467169e+02, 1.77397519e+01, 1.63487373e+03,\n",
       "       9.16074803e+02, 9.27893027e+02, 1.52344070e+03, 1.44598476e+03,\n",
       "       2.09758995e+03, 1.80733925e+00, 5.03610295e+02, 6.06383764e+02,\n",
       "       2.44795766e+03, 3.34264912e+03, 1.41191138e+03, 7.16927310e+00,\n",
       "       5.18213150e+03, 1.87276639e+03])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from __future__ import print_function, division\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, rows):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 1)\n",
    "        self.latent_dim = 1000\n",
    "        self.disc_loss = []\n",
    "        self.gen_loss =[]\n",
    "        \n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates note sequences\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated_seq = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated_seq)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
    "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        seq = Input(shape=self.seq_shape)\n",
    "        validity = model(seq)\n",
    "\n",
    "        return Model(seq, validity)\n",
    "      \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.seq_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load and convert the data\n",
    "        notes = get_notes()\n",
    "        n_vocab = len(set(notes))\n",
    "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Select a random batch of note sequences\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_seqs = X_train[idx]\n",
    "\n",
    "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
    "            #noise = (noise-242)/242\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new note sequences\n",
    "            gen_seqs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            #  Training the Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, real)\n",
    "\n",
    "            # Print the progress and save into loss lists\n",
    "            if epoch % sample_interval == 0:\n",
    "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "              self.disc_loss.append(d_loss[0])\n",
    "              self.gen_loss.append(g_loss)\n",
    "        \n",
    "        self.generate(notes)\n",
    "        self.plot_loss()\n",
    "        \n",
    "    def generate(self, input_notes):\n",
    "        # Get pitch names and store in a dictionary\n",
    "        notes = input_notes\n",
    "        pitchnames = sorted(set(item for item in notes))\n",
    "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "        \n",
    "        # Use random noise to generate sequences\n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "        predictions = self.generator.predict(noise)\n",
    "        \n",
    "        pred_notes = [x*242+242 for x in predictions[0]]\n",
    "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
    "        \n",
    "        create_midi(pred_notes, 'gan_final')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.disc_loss, c='red')\n",
    "        plt.plot(self.gen_loss, c='blue')\n",
    "        plt.title(\"GAN Loss per Epoch\")\n",
    "        plt.legend(['Discriminator', 'Generator'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  gan = GAN(rows=100)    \n",
    "  gan.train(epochs=5000, batch_size=32, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.172230, acc.: 29.69%] [G loss: 0.713075]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": " Error while reading resource variable _AnonymousVar56 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar56/N10tensorflow3VarE does not exist.\n\t [[node mul_56/ReadVariableOp (defined at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_5993]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3c557f42a19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-3c557f42a19f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Train the discriminator (real classified as ones and generated as zeros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3625\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3627\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m:  Error while reading resource variable _AnonymousVar56 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar56/N10tensorflow3VarE does not exist.\n\t [[node mul_56/ReadVariableOp (defined at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_5993]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "#         fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=4000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "# x=[4.514035720763564314e+03,4.627780786182374868e+02,1.020945567298307651e+03,1.070354155058161268e+03,5.533543765011305368e+03,7.590369377976319811e+03,2.954948118591055390e+02,1.466116552641607996e+03,4.295811526119941846e+02,3.037189918146104901e+02,2.405418348353123292e+00,1.099691661965771345e+02,2.415395419985645276e+03,9.602713470572780352e+01,2.637622617742672446e+02,3.883147210605675355e+02,4.103439594055525959e+02,3.551888462248534779e+02,8.538832430593902245e+01,5.905625803497459856e+02,2.756134854054544121e+02,3.337264630229779868e+03,8.452409671609348152e+01,2.437427593082102248e+03,9.041067672946956009e+02,2.004874771996401250e+02,1.970176135431684088e+03,3.962430043657761416e+03,7.125869599815923721e+02,4.855300338300876319e+02,3.314717316398891853e+02,1.059469240046746563e+01,4.242414881729418994e+02,6.540902069531803136e+02,8.888835177637010929e+02,1.905628445429269050e+03,1.939418279724559397e+02,2.409704104248448857e+02,1.171866432832612190e+02,3.603392362044833135e+02,3.392229437393543776e+02,4.800876806979213143e+02,1.868073045227356488e+02,2.047662194704484136e+03,9.335884379670460476e+01,6.273468676169286482e+02,5.176326141159806866e+02,8.716325789734837599e+02,1.899757382977986708e+02,3.242648889347765362e+01,3.001338812762398447e+03,1.139496040585494484e+03,3.619060527434339747e+01,2.357869184323644731e+02,1.580632874724105932e+02,6.886942157018784201e+02,7.261416926722158678e+02,7.332367527834576322e+02,3.641520747695176397e+02,8.100511807978764409e+02,3.435326495320623508e+02,1.690902797880567960e+03,5.928782385333179263e+02,6.515067413727665553e+02,8.467909570205956697e+02,3.664815713596472051e+02,7.888356199778354494e+02,2.540862429916014662e+03,2.311798521774588153e+03,1.156491994587147201e+04,8.702659064975305228e+02,1.378849131578463130e+03,8.781301760279166047e+02,4.899542435430368641e+03,1.762194796834955923e+01,5.304347161584009882e+02,2.954088241670877323e+03,6.556641254791320534e+02,2.527520428248040844e+03,5.082523811404826120e+02,1.273265805216215085e+02,2.719441184785100631e+03,1.076596799920895137e+03,1.525972824835625943e+03,6.790465513993549393e+02,1.914899587620180682e+03,6.180996023298212094e+02,3.349306768108799588e+02,3.622473592098976951e+02,1.898613516426557908e+02,1.097605728838243522e+03,5.713236811690876493e+02,1.790149277676973725e+02,9.734968373278388754e+02,7.004977495111961616e+02,8.814141016272769775e+02,7.158871537680242909e+02,1.809466204780619591e+03,1.501168348329578293e+03,1.188461714318356826e+03,6.460041785302746575e+02,1.907131076148507418e+02,1.698771777267975267e+02,6.098369566619076068e+02,1.166939044858489069e+03,1.150211613165010931e+03,5.164379453997244127e+02,1.986817527438252000e+03,2.545918901334225666e+02,2.207884136669919826e+03,2.084374972916266415e+03,2.345823669692035764e+02,6.263489316216146108e+02,2.889527510898187757e+01,3.528329778349143453e+02,3.255984583902172744e+02,1.460467251021385891e+03,1.698072249128017575e-01,4.613832089953939430e+02,1.124346450374810956e+03,2.152490253082942218e+02,1.132284235596336657e+03,1.508698308151680976e+03,9.066797533264616504e+01,3.415347696267708670e+02,1.328122311313054524e+02,1.724447627658664715e+02,3.457992699387250468e+01,1.052676101019460475e+03,2.821636285328422673e+02,4.789414677410968579e+02,1.313472388279507868e+03,1.002996961087454110e+03,6.826412439288105816e+02,5.483831797902821563e+02,4.167605321343144169e+03,3.246161422948352993e+01,6.409270781479426660e+01,3.461576318026782246e+03,1.276960499419161351e+03,1.832854374675662257e+03,1.244247940773231676e+03,1.618160881987423636e+02,1.339542167342442553e+02,1.607073702674562810e+03,2.001484968064760324e+02,8.284450839911005460e+01,2.066421832420950523e+03,6.706515818813932128e+02,1.625680882008571643e+03,4.340712544186681043e+02,1.825807617754326202e+02,2.253831429743731860e+02,5.153518893284490332e+02,1.083830088635237189e+03,3.706201705317944288e+02,3.230852592153532896e+02,2.308305738840135746e+02,4.844912676578387618e+02,4.936684104982996359e+02,2.479634970144834369e+02,5.845089472446707077e+02,3.445180908468319103e+02,2.997544425553642213e+02,9.271339487755612936e+02,1.420842508945555892e+03,1.689639170160196954e+03,8.027711526587954722e+02,2.243283487407345092e+03,6.743882763420988340e+02,3.625110717391362414e+02,8.744701840225025080e+02,1.564875939774676226e+03,1.292449442786164582e+02,2.275970795388566330e+02,4.299019908123009373e+02,9.536760389205301180e+01,1.038448294767062180e+03,3.838891148541006260e+01,5.265925297065696213e+02,6.964564760752546135e+02,1.447708748379343888e+03,2.093611843797581969e+03,3.452833311536815017e+02,2.295524548738321755e+02,3.517710376985196490e+02,1.087027129399153637e+03,2.585590013562090462e+03,1.071859660806774627e+03,2.234557444473641226e+03,2.639447791806596797e+02,3.813074002774665132e+02,2.596610621594300028e+02,2.329551177133544115e+03,6.522078014508588240e+02,5.514512655635480769e+02,4.305429484365158714e+02,1.056566368161002174e+03,2.011572868056828156e+02,4.731197271125856787e+03,1.049734920298738871e+03,4.645665962755447254e+01,4.840487307253060862e+03,5.134175032427301630e+02,1.499624261003336869e+02,1.221189358968869783e+03,3.981706250391725916e+02,1.432167868819669820e+01,3.306036813883110881e+02,6.354951902462053113e+02,7.774318470693542622e+02,1.128596305550046964e+03,1.858022182770946529e+02,2.639640925376443192e+02,9.747561133364797570e+01,6.781908678071631584e+02,6.894310467991745099e+02,8.196446086671203375e+02,8.648903985919896513e+02,1.979587345937034115e+01,5.116306763486645650e+02,2.157100286354252603e+02,8.545340037029236555e+02,8.024415675493655726e+02,9.326213627420074772e+02,8.359557416639290750e+02,1.103259605555213057e+02,3.640638279747130582e+03,1.894465161109517794e+03,2.012994121490046382e+02,2.280180167076759972e+03,5.779021605422574794e+03,1.145041307274368592e+01,5.633432444715581369e+02,7.853117061418015510e+02,4.140531079811044037e+02,2.920177636296139099e+02,3.140269567494397052e+02,3.936067386095819529e+02,7.613171728881716263e+02,3.010649196279991884e+02,9.744394628680311143e+00,2.537170649358886294e+02,2.010553649145876989e+02,7.906695144966943190e+01,4.012836914883810095e+02,1.560698782434046734e+02,1.660791211409377865e+01,2.431413982641533948e+02,2.029338515923009254e+02,4.928263396921975072e+02,9.959913561528082937e+00,1.328747729411814362e+03,3.067678470183745958e+02,8.232552385373855941e+01,9.725630861532408744e+01,7.847180415185284801e+02,1.506920286467939150e+02,2.429699662883649580e+01,1.800181569811829831e+02,7.532825213627947960e+02,6.958000551356235519e+00,4.736777570446429309e+02,1.373237444265105296e+02,1.172011551639909158e+03,4.691875248705036938e+01,8.375585463448078372e+01,4.680986876276438124e+01,6.048515789912198670e+01,3.517705856486863922e+02,1.259530724877840839e+02,5.812656322001712397e+02,2.396123797768959776e+02,5.796993960912222974e+02,2.033441224546986632e+02,8.117440653823432513e+02,5.520299457741202787e+01,8.017464614785858430e+02,1.148397756643535104e+02,1.448192456427204888e+02,1.985817126771144103e+02,2.252550864406803157e+02,3.928333729008736555e+02,3.974837232328427490e+02,1.291631600745313335e+02,7.393500322971667629e+02,2.949208194112870842e+02,1.638526550561015029e+03,1.531186621233879123e+03,1.172115318119875155e+03,5.392293946337886155e+02,3.505681473026052117e+02,5.337368133759591728e+02,5.071718002289999276e+02,7.253002092700335197e+02,9.347005189087940380e+01,8.800398246045806445e+02,2.418228135848068632e+03,1.167282942915917374e+03,1.584569533932721242e+03,3.456241740082623437e+02,4.600443391027278267e+03,2.102278372648288496e+03,3.651104458191548474e+02,5.605815065447182860e+03,7.990194910013349727e+03,1.409949904383800458e+03,7.445638533213059418e+02,5.645190171285066754e+02,8.556593353245407343e+02,6.965583224734291434e+01,1.081506203301483765e+03,8.895273944073705934e+02,2.436118316561332904e+02,5.544482259940123186e+02,4.887831253553740680e+02,1.214338964314665645e+02,4.095114040703629144e+02,9.452153902581194416e+02,2.855069600146380253e+02,2.483983593486482278e+01,1.562434148993925191e+02,6.875831165833515115e+02,2.587248381890822202e+02,7.847806990759563632e+02,3.683644838363397866e+01,9.534360750142950565e+02,8.390103066980373114e+01,6.990721378277521580e+01,9.999739448503823951e+02,3.583268854090711102e+02,7.604801400345750153e+02,8.625940131151583046e+01,6.874050769052701071e+01,9.091544837999390438e+01,6.885531801416072994e+01,3.859944241282646544e+02,3.134115283312276006e+02,3.841718697138130665e+02,6.383395288954488933e+00,3.430786835396429524e+01,4.610968222155352123e+02,6.127850417184527032e+02,1.795325291467015631e+02,7.266612505602533929e+02,1.013051197680470068e+03,2.580720412032678723e+03,6.420088288438273594e+02,3.269687427291646600e+02,5.381510709480498917e+02,1.036214860595995560e+03,3.640336307911202312e+02,9.538444093830185011e+01,8.219061742202611640e+02,2.671831197148421779e+02,4.320767708161729388e+02,5.225289318296127021e+02,8.359596907207742333e+02,7.513527027864474803e+02,6.167111120847868733e+03,7.799018970204051584e+02,2.261381779065937735e+03,4.451071156072430313e+00,9.896173455375246704e+02,5.450433489513816312e+02,2.161837024876556825e+03,6.569173902203910984e+02,6.633270198926329613e+02,1.084291128222655971e+03,2.252149519236409105e+02,4.440111581960809417e+02,1.405924125719117001e+03,4.182735622033942491e+01,1.009681621590279974e+02,1.127791247456683777e+03,3.003133427696884610e+02,4.007552133270655759e+02,9.603262389513547532e+02,3.449046510593616404e+02,5.706726496643386781e+01,2.530473042579833418e+02,3.071522011794149876e+01,6.813119602706283331e+02,6.181789611568092369e+02,4.913717925205128267e+01,5.452515375112416223e+02,1.540264809486689046e+02,1.392627450444852002e+02,3.423049778555287048e+02,2.306517663956037723e+02,2.339069741083076224e+02,1.236583820095984265e+01,3.169503358512884006e+02,7.490049123030621558e+01,3.910108948052511550e+02,3.295484657705528662e+02,4.851950346740195528e+02,9.533350034470204264e+02,1.527332359007559717e+02,1.391772460866486654e+02,6.012288692597649060e+02,1.656365534957731143e+02,1.574592398823006079e+02,1.852629565680399537e+01,5.432897150923963636e+01,1.227419010094599798e+02,8.478484138846397400e+02,1.636944101167027839e+02,4.009389162399456836e+02,3.568843478664639406e+02,6.579887727400055155e+02,8.560283411704585887e+02,4.286433898766408674e+02,3.584451947077177465e+02,2.301937872627167962e+02,1.746340610600309446e+03,2.362615995981614105e+02,1.136412024759338237e+02,3.878395604737452231e+02,1.707309384359803516e+03,6.148017466163728386e+02,5.523439575251541100e+02,7.135630195291014388e+01,3.319342620146926492e+01,1.899502624816959724e+02,1.863477068306528963e+02,1.420994177701475564e+03,1.165770728530536871e+03,2.514126786345732398e+02,6.343524750744109042e+02,6.311990651758387685e+02,1.461167558629531413e+02,3.631992828566581011e+00,1.005589970183849800e+03,1.898989545982913114e+02,4.457315191430971026e+02,1.726597723963786848e+02,7.886878530912217684e+02,2.075596865499974228e+02,1.383332238399307244e+02,1.380812207026174292e+02,2.664736631929758005e+03,1.021710874727519695e+03,3.374511205620365217e+01,8.240576451295055449e+01,8.430209631777834147e+02,5.128724136753589846e+02,4.563762925887422170e+03,1.336377707909559831e+02,1.511048304688883945e+03,3.065936410823429469e+03,2.650720535037864465e+03,4.806324710137560032e+02,5.483906955506536178e+02,4.249724914645776153e+02,3.425002022126573138e+02,2.047377300731022842e+02,4.163851778804091737e+02,1.550435744114220142e+02,7.907025588471442461e+03,8.112852066207560711e+02,9.021790692663635127e+02,3.737969281621626578e+03,2.222383138860575855e+03,3.814855479437101167e+03,1.757384280863392632e+03,7.466207868263591081e+02,9.127151421230519190e+02,1.954157212073798291e+03,1.084471071491373004e+04,5.352122346702264622e+03,7.726608758135698736e+02,7.636673052014666609e+02,6.016625992365879938e+01,3.342790615825331770e+03,5.693820764936390333e+02,3.578401637758885045e+03,7.039262283232528716e+01,1.408584258295659674e+04,3.247406030019395985e+03,5.643931431117409375e+03,4.636362591321812943e+01,2.053410605612327345e+03,2.000401386734680273e+03,8.169432265749201179e+02,2.946323516024276614e+02,9.752191460113972425e+02,1.602206554892705753e+03,2.580692696308018640e+03,2.474129394141666126e+03,3.003151634518522769e+03,9.058521468379767612e+02,2.767384541453793645e+01,2.525418811813520733e+03,3.304193350124405697e+02,2.352892589275725186e+02,1.708549472723621875e+03,3.214953822927782312e+02,5.097583610173314810e+02,1.440838336684391834e+03,5.464057725082966499e+03,9.577147078240057454e+03,2.055985286236973479e+03,2.614625594804529101e+03,9.720713788778986782e+03,1.089514118193881586e+03,3.180083555578778032e+03,6.201026614823436830e+03,1.317976788832223974e+03,1.595595367119298317e+03,3.365426187380217016e+02,1.111021880178130232e+03,3.369068327356362715e+02,1.847254513814114034e+02,3.877444936422398314e+03,6.678795424220152199e+01,5.164190368722192943e+02,1.511555455790366977e+02,1.071246625886764377e+03,3.772490734081482515e+03,8.003169649918563664e+02,6.752085207925643772e+03,1.682788229259196669e+03,5.567121454157051630e+03,5.509167628231691197e+03,2.064657360284239985e+03,3.877079347229213454e+03,7.039377342090941966e+03,1.429524017087765969e+03,2.639515991674619727e+03,7.942963476432487369e+02,4.803567479936173186e+02,3.118423914614948444e+03,1.381668763416819274e+02,1.426494639274082147e+03,3.576695782643510029e+02,6.253179099434055388e+02,1.470126788145862520e+02,6.840660667808260769e+02,1.344288468536804430e+03,3.333220268366858363e+02,8.223110156343318522e+01,8.856782370752189308e+02,1.716034861674648710e+03,4.079967965286923572e+02,3.156530868620611727e+02,1.665540015056496486e+03,9.201640527624404058e+02,5.781281116316094995e+02,7.577248539539286867e+02,6.230386264882981777e+02,8.965259130013873801e+02,3.243383987458655611e+02,1.073524057998787612e+03,1.380101018253481016e+02,2.342808722091140226e+03,3.740946601088857278e+03,5.278658012090018019e+02,1.266811286036856472e+03,4.185888718520873226e+03,1.371820583469117992e+03,4.326565261051291600e+02,2.768041229902184568e+03,1.227084422054584138e+03,1.104124662005458958e+03,1.214053137396695092e+02,2.786799437989131548e+03,2.305913477807538584e+03,5.955651503590634093e+02,7.974656832867767662e+02,5.176538139964686707e+02,7.128353422244545072e+02,5.078783585423370823e+02,2.326134222074761055e+03,9.949778653462417424e+02,2.665684022931382060e+02,1.323620487753767520e+03,1.078256848909892142e+03,1.188035205610794947e+02,1.252135452629416250e+03,5.958914294436108321e+02,1.068730056819273159e+03,5.005014879700029269e+02,1.595949313548393548e+03,6.221769159904215485e+01,1.739341270708246157e+02,1.090614061593310907e+03,8.269677258097799495e+02,4.769695420777425170e+02,3.014808114786865190e+02,2.038193515538587235e+03,2.522931249671382830e+02,6.674680568458279595e+02,5.309193604049505666e+02,7.880229289742419496e+02,1.851519429363310337e+03,3.336230206254404038e+02,4.457115161843830720e+02,6.392722413537558168e+01,9.379357078898465261e+02,7.090655713943997398e+02,1.639427018250338733e+03,9.947675732176285237e+02,6.706779863480478525e+02,4.154180429955013096e+02,1.261405400944175199e+03,4.035079789080191404e+02,2.487276577290962450e+03,4.721905315542826429e+02,4.919080760086653754e+02,2.873670744857867248e+03,3.635397382522933185e+01,8.855609568214276806e+02,6.508324645218672231e+02,1.563891722811269574e+03,8.718873322204453871e+02,3.332360558281652629e+01,3.738881397718796507e+02,3.321646512930747122e+02,3.205986535261617973e+02,1.193855381279718131e+03,2.030575105973985046e+02,1.502924802161287516e+02,5.175333132711239159e+01,8.284654938152525574e+02,9.466009470012504607e+02,8.734095504852011800e+02,2.710061979102902114e+02,6.291251175121869892e+02,1.996260352552519180e+03,1.026144018757855520e+03,2.862280237363651395e+03,1.844440043073846027e+02,1.728755174492835067e+03,6.658063626637449488e+02,3.408519302431959659e+02,1.899842732596443966e+02,4.201567537323571742e+02,8.800221201562089846e+02,4.939664965744595975e+03,1.217021633588243276e+03,1.006250359194236808e+03,1.676627846552291885e+02,9.629653233571443707e+02,1.102444257457391359e+03,9.126451553590595722e+01,3.262890948562300764e+03,1.592684036238351837e+02,3.280017602327279747e+02,8.384564007037552074e+02,7.260753818647935987e+01,3.193349609151482582e+00,6.408090186854824424e+02,2.402632580604404211e+01,1.783840088868746534e+02,2.941500443643890321e+02,3.634404245589394122e+01,4.131452004495076835e+01,5.771470279658678919e+02,3.306524124455871060e+03,7.565808005437720567e+02,3.585250467025907710e+02,1.266723879162687808e+03,7.288688666104571894e+02,9.772565860766917467e-01,1.947215357248205692e+02,5.301818058434873819e+02,3.880105669328477234e+02,4.288317570998333395e+01,1.331095862216316164e+02,1.258103281109360978e+02,6.201673464953200892e+02,7.094028560870792717e+02,1.213666112302104011e+03,1.896239648419432342e+02,3.002282629949040711e+02,1.952072952548041940e+01,7.090636159828864038e+01,1.629090987838339061e+03,6.176732402120251209e+02,1.710460136185865849e+02,5.957990180719643831e+02,1.458065728482324630e+02,1.012681384698953480e+01,9.019300358684267849e+01,1.413899347900878638e+02,4.149342284840531647e+02,1.268301026012049988e+02,2.910270768502959982e+02,1.080670470876386389e+03,3.976065976044628769e+01,2.581672358646756038e+02,1.888068985771387815e+02,1.139276868510525674e+03,5.465361379680689424e+01,7.162420761405956000e+02,1.623719272983726114e+02,2.083785486500710249e+01,4.624779136882862076e+02,7.386229448858648539e+02,1.190879127623047680e+02,2.373389523979276419e+02,1.284568338985554874e+03,6.986040078937076032e+02,1.281340617341920733e+02,7.361808659264352173e+01,2.414733448846964166e+02,8.919193803686648607e+02,1.354817001199349761e+03,8.662187150772660971e+01,4.339434947153786197e+02,3.820492149865021929e+02,3.605387502515222877e+02,1.512370822964934632e+03,7.677477004327811301e+02,2.212247906515840441e+02,1.948286008015275002e+02,1.711494351888541132e+03,4.063397326346021146e+02,7.922586347220931202e+01,1.725797217268263921e+02,8.858477138518355787e+02,2.991435606012819335e+02,2.650076512361411005e+02,6.159449955101590604e+02,2.271282027394045144e+02,4.517577697833767161e+02,2.528175453209551051e+02,1.421261589376372285e+03,2.860167204536264762e+02,2.264047327800653875e+00,3.669938058123225346e+02,1.311219831230351701e+03,6.095454475474543869e+02,8.424113501717802137e+02,4.888442379313055426e+02,4.577583936128066853e+02,2.473853212097892538e+02,2.146157151999650523e+02,3.668747366163879633e+02,6.962737543864641339e+02,5.794867799966596067e+00,1.170513603350846097e+02,4.648980381679721177e+02,3.121300021989736706e+02,3.691558959509711713e+02,1.258611480573890731e+02,2.702032876368612051e+02,3.209028438186505809e+02,2.003314302074722946e+02,8.872801950178109109e+02,8.507352691786363721e+02,1.469062750437296927e+01,4.040504642548039556e+02,1.712366238103713840e+03,3.990584956886013970e+02,3.426114053671481088e+02,8.291263021748745814e+02,3.791272161587839946e+02,2.288641382406814955e+03,1.232158485995256342e+03,3.410305142381694168e+02,1.089095491754822433e+03,7.119059710053261369e+01,3.385694799195043743e+02,6.164975862915161997e+01,3.799946583837736398e+02,6.110993875244166702e+02,6.197383271086728200e+02,7.181621567574329674e+02,3.987568090459099039e+02,2.767958025743719190e+02,2.208141638575936668e+03,2.809053443688899279e+02,5.812675736809615046e+02,1.050523799286806025e+03,8.367444877687375993e+01,8.143353185312589630e+02,5.875086532779969275e+02,3.948239024800714105e+02,2.180520395515486598e+01,7.319400809870567173e+02,3.272153195198625326e+02,1.286731742604169995e+02,6.425518129968550056e+02,2.975551162414485589e+02,1.692837857763515785e+02,6.139193472976330668e+02,5.438932529877638444e+02,4.178857257418567315e+02,6.103836513857822865e+01,6.617257887759478763e+02,2.993265994754619896e+02,5.914521722903009504e+01,2.088112967819790356e+03,4.087630764965433627e+01,2.515240571871981956e+03,1.764181120435241610e+02,1.123123143319739029e+03,2.180956873320974410e+01,2.307541528791189194e+02,3.382524313968606293e+02,9.904065179205499589e+02,7.931578855791594833e+02,2.461585105265257880e+02,1.861975376926129684e+02,1.665616260198876262e+03,1.273900514381239191e+03,9.709242729910183698e+02,1.503191593172959983e+03,1.012339591948431917e+03,5.804949493235908449e+02,1.370246273421216756e+02,1.318321834888774902e+02,1.560056395370163955e+03,4.015337818304542452e+02,2.793621538778534159e+02,3.046918467503273860e+02,2.302902878897730261e+02,3.249027089229784906e+02,2.124904380530351773e+02,3.674338215511525050e+02,2.203227923637023196e+02,8.400074181205127388e+01,4.178053909983718768e+02,1.161778475216124207e+01,1.043845162544166669e+02,1.289678225802490488e+02,2.657457660487852991e+01,2.310355658989865333e+02,3.299200614334549755e+02,4.022284566133748740e+01,1.596162785351043567e+02,1.762020232037175447e+02,5.944799861032515764e+01,1.259687859227415174e+02,9.853562134181847796e+02,5.797768809100380167e+02,4.855298052529105917e+02,3.448456183803500608e+02,7.914124817994888872e+01,6.137703214504290372e+01,1.670000942740589380e+02,5.420697338738245890e+02,7.123551299329847097e+01,4.583906833863584325e+02,2.805215862096520141e+02,1.146627587453695014e+02,1.416335157435736619e+03,2.841836448289686814e+02,4.028478826908394694e+02,2.350719019524985924e+02,4.060421613056678325e+01,4.726002174965105951e+02,6.011535905848722905e+01,2.685319313646759838e+02,5.961708309652749449e+01,9.368044126126915216e+01,1.533187262369319797e+02,1.580600912356749177e+02,1.213456565777305514e+02,4.259226538846269250e+01,8.337241356493905187e+00,9.019259564545936882e+02,3.646452356420923024e+01,1.317373174407985061e+02,4.525165039545390755e+01,1.086486204341053963e+01,4.937930491361767054e+02,8.023265969532076269e+02,1.646439768320415169e+02,5.136897966521792114e+01,8.020529248800594360e+02,1.481299321085680276e+02,2.750717984740622342e+02,5.226869923644699156e+00,3.213570832862751558e+02,9.899724032732192427e+02,9.301049515767954290e+01,3.965183902839198709e+02,3.756488053477369249e+00,3.884481859328225255e+02,7.990109641267918050e+01,2.765895432771649212e+02,1.393744270505849272e+02,1.465899945701239631e+02,1.682840774136129767e+02,6.402543799264822155e+01,1.866015833913930692e+03,7.616233606657478958e+02,1.764160726100672036e+02,8.049509758676867932e+01,2.073125982512719929e+02,1.806452051419764757e+02,5.318390422163065523e+01,4.166758801310788840e+01,2.212946882987162098e+02,2.123608428447041661e+02,1.099517568387091160e+01,8.767020363849587739e+01,1.536568526586052030e+02,2.673785017535556108e+01,1.735221819940488786e+01,1.060445463889045641e+02,1.625976818043272942e+01,1.546953807452227920e+02,1.957485451158136129e+02,8.098433326580561697e+01,4.742774183251895010e+02,1.016856944824336097e+03,3.702464712178334594e+01,6.295732955716084689e+01,3.556376285420265049e+01,1.071309720133431256e+02,1.067743805777630769e+03,1.319879187849233858e+03,2.928029943180736154e+02,1.566723162947106175e+03,1.730708684684941545e+02,4.127067354742903262e+02,3.568307499319780618e+02,1.314015581246931106e+02,3.150216657950310037e+03,7.913675373113946989e+02,4.051246247155359015e+02,2.207964028441347182e+01,1.494900623191148043e+03,1.154596344135934487e+02,3.389039248175686225e+02,5.819274774022633210e+02,3.559931307578226551e+02,3.706049405087949708e+03,9.459206038387492299e+00,1.619390547231305391e+02,1.517876301526557654e+03,1.587511495175072923e+03,8.620254842140711844e+02,1.001863796177785844e+03,7.940719012304907665e+02,8.652210541197564453e+01,3.393486680850619450e+02,1.288711727887974121e+03,5.235396450196858495e+02,1.652721491161501035e+02,1.250896959548117593e+02,1.485024743617745116e+03,1.168254290466778912e+03,2.451372980752494186e+02,2.108576602587709203e+02,2.372104019726859406e+03,1.872766405042493716e+02,5.233629763011122122e+02,9.209884229088202119e+02,7.858090132205979899e+02,4.618261733663966879e+02,8.187512114490382373e+02,4.752130484995432198e+02,3.129442923342459835e+03,6.281320393516216427e+01,4.372915679638972506e+02,4.040277094736229628e+02,5.207521821180125698e+02,1.383178952435730025e+03,4.829899201665539294e+02,1.160552768755238503e+03,1.380778460040455684e+03,1.417958584606647491e+03,5.196950594371883199e+02,2.843801214378559962e+03,9.623811363428831100e+01,5.436904078306397423e+02,1.018279823722550645e+02,8.953881027910392731e+01,5.318700724869268015e+02,8.938435639264062047e+02,2.601157362700905651e+02,3.812450302549405023e+02,4.653044847133569419e+02,5.662603472326882184e+02,2.239861202395055443e+03,1.347959732894087210e+02,4.186386130700120702e+02,1.648999046306125820e+02,2.337226168124238029e+03,2.173169491208391264e+02,6.282519292373908684e+02,4.741522437245585024e+02,4.358242099705385044e+02,2.976935920852702111e+01,1.185265226662158966e+01,3.069007186731323600e+01,3.474262966657988727e+01,1.017381584175280295e+03,3.749516128401737660e+02,3.280664962503360584e+02,5.714322486934252083e+02,1.408393102986272424e+03,1.028153707010205835e+02,7.067952815640019253e+02,6.781510392258642241e+02,9.247201137035153806e+01,3.801582057415507734e+02,3.120645441638771445e+02,1.041616836668225005e+03,8.973610398506280035e+02,1.920180301845539361e+02,2.356353083164431155e+02,3.419723560113925487e+02,3.281563626069109887e+02,3.234377265397924930e+02,1.682627564077265561e+02,2.031985830853227526e+02,2.327612483687698841e+02,6.855018801904516295e+02,2.023634182926034555e+02,3.156316762082278728e+02,2.654466390797169879e+02,4.000738650610437617e+02,7.288660895476350561e+02,6.522549226018600166e+01,6.284411861142143607e+01,1.422270912128733471e+02,1.304546245769597590e+01,1.200134860058315098e+02,6.885019591770833358e+02,8.827684683079132810e+02,7.988072171111125499e+02,2.143928416538983583e+00,3.623751241229474545e+02,3.936697511314996518e+03,1.250416369558311999e+03,6.229481967417523265e+02,8.167416631007799879e+02,3.158993896622443572e+03,2.681035822004312649e+03,4.502622836653608829e+01,3.648576472956920043e+02,4.434322584036272019e+01,1.142969728514668532e+03,6.022815788420848548e+02,1.850974977559759282e+03,3.199131724471226335e+02,2.293696089395089075e+03,5.527744474824285135e+02,5.661572558307088912e+02,5.967287963256239891e+01,3.677147250541020185e+02,7.341862188472878188e+02,2.018625054522417486e+02,1.340249904675874859e+02,8.188881266918033361e+02,2.684376175458310172e+02,6.613509828329551965e+01,5.764663849162170663e+02,5.334794707226101309e+02,2.614213000914314762e+02,2.896376412167446688e+02,8.979756207787431777e+00,1.103924460844136775e+02,1.453803781812079251e+03,6.630019148138817400e+02,1.794895483712898567e+02,1.061346408139215782e+02,2.707722363110398874e+03,1.004338032015948556e+03,1.055284710474894382e+03,8.145432749383617193e+02,9.476031024358235300e+01,1.424766561895608902e+03,1.245436804932774976e+03,1.160782990045845509e-01,1.235986692383885384e+02,5.379390223473310471e+02,6.778570872730342671e+02,5.472622508398490027e+02,6.339218740027863532e+02,3.400926072958391160e+02,5.361879429141990840e+02,1.351091415001545101e+03,5.543622272559441626e+02,2.507410847570979968e+03,1.999212610301095992e+03,4.045387350196251646e+02,4.117234376472188160e+02,6.066295394068583846e+01,5.292532532161567360e+01,9.258453455753624439e+02,1.130608880178770050e+02,1.166283442875836045e+03,5.004667685074964538e+02,1.498297533616656438e+03,9.868267967872088775e+02,1.073498464558389969e+03,2.121164016814786009e+03,9.852452967759454623e+02,3.383426055712625384e+02,4.251952645347919315e+02,1.530115335633745417e+02,3.475366139421472326e+03,7.270382242412306368e+02,2.201487038129242137e+03,2.742183506674482487e+03,5.691765096116578206e+02,1.078410814861999825e+03,1.644850617309566587e+03,5.668525741640478373e+02,2.726105661431443878e+03,9.902182811907259747e+02,3.150503233015304431e+03,6.870669298515422270e+02,4.242614038654603064e+03,2.558508081055479124e+02,4.421481423558318056e+03,6.191733880236279219e+02,7.694372498979908414e+03,2.054855137862032279e+02,4.674463524763705209e+02,4.649560111855389550e+02,9.752509211235446855e+02,1.467751919926959090e+03,1.076064661723794416e+02,8.716901212019147351e+02,1.256520727291353978e+03,1.806200162361958064e+03,5.698299217238556594e+03,1.872911685516941361e+03,1.301192655249731615e+03,2.670532133898581378e+03,5.733901106007397175e+01,5.160812607174739242e+01,1.491662885941797867e+03,1.894599938718369231e+03\n",
    "# ]\n",
    "n_vocab = len(set(x))\n",
    "def prepare_sequences(seq, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(seq) - sequence_length, 1):\n",
    "        sequence_in = seq[i:i + sequence_length]\n",
    "        sequence_out = seq[i + sequence_length]\n",
    "        network_input.append(sequence_in)\n",
    "        network_output.append(sequence_out)\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # Normalize input between -1 and 1\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n",
    "n_input, n_output = prepare_sequences(x,n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4514.035720763564, 462.7780786182375, 1020.9455672983077, 1070.3541550581613, 5533.543765011305, 7590.36937797632, 295.49481185910554, 1466.116552641608, 429.5811526119942, 303.7189918146105, 2.4054183483531233, 109.96916619657713, 2415.3954199856453, 96.0271347057278, 263.76226177426724, 388.31472106056754, 410.3439594055526, 355.1888462248535, 85.38832430593902, 590.562580349746, 275.6134854054544, 3337.26463022978, 84.52409671609348, 2437.4275930821022, 904.1067672946956, 200.48747719964013, 1970.176135431684, 3962.4300436577614, 712.5869599815924, 485.53003383008763, 331.4717316398892, 10.594692400467466, 424.2414881729419, 654.0902069531803, 888.8835177637011, 1905.628445429269, 193.94182797245594, 240.97041042484489, 117.18664328326122, 360.3392362044833, 339.2229437393544, 480.0876806979213, 186.80730452273565, 2047.6621947044841, 93.3588437967046, 627.3468676169286, 517.6326141159807, 871.6325789734838, 189.97573829779867, 32.426488893477654, 3001.3388127623984, 1139.4960405854945, 36.1906052743434, 235.78691843236447, 158.0632874724106, 688.6942157018784, 726.1416926722159, 733.2367527834576, 364.15207476951764, 810.0511807978764, 343.53264953206235, 1690.902797880568, 592.8782385333179, 651.5067413727666, 846.7909570205957, 366.4815713596472, 788.8356199778354, 2540.8624299160147, 2311.798521774588, 11564.919945871472, 870.2659064975305, 1378.8491315784631, 878.1301760279166, 4899.542435430369, 17.62194796834956, 530.434716158401, 2954.0882416708773, 655.664125479132, 2527.520428248041, 508.2523811404826, 127.32658052162151, 2719.4411847851006, 1076.5967999208951, 1525.972824835626, 679.0465513993549, 1914.8995876201807, 618.0996023298212, 334.93067681087996, 362.2473592098977, 189.8613516426558, 1097.6057288382435, 571.3236811690876, 179.01492776769737, 973.4968373278389, 700.4977495111962, 881.414101627277, 715.8871537680243, 1809.4662047806196, 1501.1683483295783, 1188.4617143183568, 646.0041785302747, 190.71310761485074, 169.87717772679753, 609.8369566619076, 1166.939044858489, 1150.211613165011, 516.4379453997244, 1986.817527438252, 254.59189013342257, 2207.88413666992, 2084.3749729162664, 234.58236696920358, 626.3489316216146, 28.895275108981878, 352.83297783491435, 325.5984583902173, 1460.467251021386, 0.16980722491280176, 461.38320899539394, 1124.346450374811, 215.24902530829422, 1132.2842355963367, 1508.698308151681, 90.66797533264617, 341.53476962677087, 132.81223113130545, 172.44476276586647, 34.579926993872505, 1052.6761010194605, 282.16362853284227, 478.94146774109686, 1313.4723882795079, 1002.9969610874541, 682.6412439288106, 548.3831797902822, 4167.605321343144, 32.46161422948353, 64.09270781479427, 3461.5763180267822, 1276.9604994191614, 1832.8543746756623, 1244.2479407732317, 161.81608819874236, 133.95421673424426, 1607.0737026745628, 200.14849680647603, 82.84450839911005, 2066.4218324209505, 670.6515818813932, 1625.6808820085716, 434.0712544186681, 182.58076177543262, 225.3831429743732, 515.351889328449, 1083.8300886352372, 370.62017053179443, 323.0852592153533, 230.83057388401357, 484.49126765783876, 493.66841049829964, 247.96349701448344, 584.5089472446707, 344.5180908468319, 299.7544425553642, 927.1339487755613, 1420.842508945556, 1689.639170160197, 802.7711526587955, 2243.283487407345, 674.3882763420988, 362.51107173913624, 874.4701840225025, 1564.8759397746762, 129.24494427861646, 227.59707953885663, 429.90199081230094, 95.36760389205301, 1038.4482947670622, 38.38891148541006, 526.5925297065696, 696.4564760752546, 1447.708748379344, 2093.611843797582, 345.2833311536815, 229.55245487383218, 351.77103769851965, 1087.0271293991536, 2585.5900135620905, 1071.8596608067746, 2234.557444473641, 263.9447791806597, 381.3074002774665, 259.66106215943, 2329.551177133544, 652.2078014508588, 551.4512655635481, 430.54294843651587, 1056.5663681610022, 201.15728680568282, 4731.197271125857, 1049.7349202987389, 46.45665962755447, 4840.487307253061, 513.4175032427302, 149.9624261003337, 1221.1893589688698, 398.1706250391726, 14.321678688196698, 330.6036813883111, 635.4951902462053, 777.4318470693543, 1128.596305550047, 185.80221827709465, 263.9640925376443, 97.47561133364798, 678.1908678071632, 689.4310467991745, 819.6446086671203, 864.8903985919897, 19.79587345937034, 511.63067634866457, 215.71002863542526, 854.5340037029237, 802.4415675493656, 932.6213627420075, 835.9557416639291, 110.3259605555213, 3640.6382797471306, 1894.4651611095178, 201.29941214900464, 2280.18016707676, 5779.021605422575, 11.450413072743686, 563.3432444715581, 785.3117061418016, 414.0531079811044, 292.0177636296139, 314.0269567494397, 393.60673860958195, 761.3171728881716, 301.0649196279992, 9.744394628680311, 253.71706493588863, 201.0553649145877, 79.06695144966943, 401.283691488381, 156.06987824340467, 16.60791211409378, 243.1413982641534, 202.93385159230093, 492.8263396921975, 9.959913561528083, 1328.7477294118144, 306.7678470183746, 82.32552385373856, 97.25630861532409, 784.7180415185285, 150.69202864679391, 24.296996628836496, 180.01815698118298, 753.2825213627948, 6.9580005513562355, 473.67775704464293, 137.32374442651053, 1172.0115516399092, 46.91875248705037, 83.75585463448078, 46.80986876276438, 60.48515789912199, 351.7705856486864, 125.95307248778408, 581.2656322001712, 239.61237977689598, 579.6993960912223, 203.34412245469866, 811.7440653823433, 55.20299457741203, 801.7464614785858, 114.83977566435351, 144.8192456427205, 198.5817126771144, 225.25508644068032, 392.83337290087366, 397.48372323284275, 129.16316007453133, 739.3500322971668, 294.9208194112871, 1638.526550561015, 1531.1866212338791, 1172.1153181198752, 539.2293946337886, 350.5681473026052, 533.7368133759592, 507.1718002289999, 725.3002092700335, 93.4700518908794, 880.0398246045806, 2418.2281358480686, 1167.2829429159174, 1584.5695339327212, 345.62417400826234, 4600.443391027278, 2102.2783726482885, 365.11044581915485, 5605.815065447183, 7990.19491001335, 1409.9499043838005, 744.5638533213059, 564.5190171285067, 855.6593353245407, 69.65583224734291, 1081.5062033014838, 889.5273944073706, 243.6118316561333, 554.4482259940123, 488.78312535537407, 121.43389643146656, 409.5114040703629, 945.2153902581194, 285.506960014638, 24.839835934864823, 156.24341489939252, 687.5831165833515, 258.7248381890822, 784.7806990759564, 36.83644838363398, 953.436075014295, 83.90103066980373, 69.90721378277522, 999.9739448503824, 358.3268854090711, 760.480140034575, 86.25940131151583, 68.74050769052701, 90.9154483799939, 68.85531801416073, 385.99442412826465, 313.4115283312276, 384.17186971381307, 6.383395288954489, 34.307868353964295, 461.0968222155352, 612.7850417184527, 179.53252914670156, 726.6612505602534, 1013.0511976804701, 2580.7204120326787, 642.0088288438274, 326.96874272916466, 538.1510709480499, 1036.2148605959956, 364.03363079112023, 95.38444093830185, 821.9061742202612, 267.1831197148422, 432.07677081617294, 522.5289318296127, 835.9596907207742, 751.3527027864475, 6167.111120847869, 779.9018970204052, 2261.3817790659377, 4.45107115607243, 989.6173455375247, 545.0433489513816, 2161.837024876557, 656.9173902203911, 663.327019892633, 1084.291128222656, 225.2149519236409, 444.01115819608094, 1405.924125719117, 41.827356220339425, 100.968162159028, 1127.7912474566838, 300.31334276968846, 400.7552133270656, 960.3262389513548, 344.90465105936164, 57.06726496643387, 253.04730425798334, 30.7152201179415, 681.3119602706283, 618.1789611568092, 49.13717925205128, 545.2515375112416, 154.0264809486689, 139.2627450444852, 342.3049778555287, 230.65176639560377, 233.90697410830762, 12.365838200959843, 316.9503358512884, 74.90049123030622, 391.01089480525116, 329.54846577055287, 485.19503467401955, 953.3350034470204, 152.73323590075597, 139.17724608664867, 601.2288692597649, 165.63655349577311, 157.4592398823006, 18.526295656803995, 54.328971509239636, 122.74190100945998, 847.8484138846397, 163.69441011670278, 400.9389162399457, 356.88434786646394, 657.9887727400055, 856.0283411704586, 428.64338987664087, 358.44519470771775, 230.1937872627168, 1746.3406106003094, 236.2615995981614, 113.64120247593382, 387.8395604737452, 1707.3093843598035, 614.8017466163728, 552.3439575251541, 71.35630195291014, 33.193426201469265, 189.95026248169597, 186.3477068306529, 1420.9941777014756, 1165.7707285305369, 251.41267863457324, 634.3524750744109, 631.1990651758388, 146.11675586295314, 3.631992828566581, 1005.5899701838498, 189.8989545982913, 445.7315191430971, 172.65977239637868, 788.6878530912218, 207.55968654999742, 138.33322383993072, 138.08122070261743, 2664.736631929758, 1021.7108747275197, 33.74511205620365, 82.40576451295055, 843.0209631777834, 512.872413675359, 4563.762925887422, 133.63777079095598, 1511.048304688884, 3065.9364108234295, 2650.7205350378645, 480.632471013756, 548.3906955506536, 424.9724914645776, 342.5002022126573, 204.73773007310228, 416.3851778804092, 155.04357441142201, 7907.0255884714425, 811.2852066207561, 902.1790692663635, 3737.9692816216266, 2222.383138860576, 3814.855479437101, 1757.3842808633926, 746.6207868263591, 912.7151421230519, 1954.1572120737983, 10844.71071491373, 5352.122346702265, 772.6608758135699, 763.6673052014667, 60.1662599236588, 3342.7906158253318, 569.382076493639, 3578.401637758885, 70.39262283232529, 14085.842582956597, 3247.406030019396, 5643.931431117409, 46.36362591321813, 2053.4106056123273, 2000.4013867346803, 816.9432265749201, 294.63235160242766, 975.2191460113972, 1602.2065548927058, 2580.6926963080186, 2474.129394141666, 3003.1516345185228, 905.8521468379768, 27.673845414537936, 2525.4188118135207, 330.41933501244057, 235.28925892757252, 1708.5494727236219, 321.49538229277823, 509.7583610173315, 1440.8383366843918, 5464.0577250829665, 9577.147078240057, 2055.9852862369735, 2614.625594804529, 9720.713788778987, 1089.5141181938816, 3180.083555578778, 6201.026614823437, 1317.976788832224, 1595.5953671192983, 336.5426187380217, 1111.0218801781302, 336.90683273563627, 184.7254513814114, 3877.4449364223983, 66.78795424220152, 516.4190368722193, 151.1555455790367, 1071.2466258867644, 3772.4907340814825, 800.3169649918564, 6752.085207925644, 1682.7882292591967, 5567.121454157052, 5509.167628231691, 2064.65736028424, 3877.0793472292135, 7039.377342090942, 1429.524017087766, 2639.5159916746197, 794.2963476432487, 480.3567479936173, 3118.4239146149484, 138.16687634168193, 1426.4946392740821, 357.669578264351, 625.3179099434055, 147.01267881458625, 684.0660667808261, 1344.2884685368044, 333.32202683668584, 82.23110156343319, 885.6782370752189, 1716.0348616746487, 407.99679652869236, 315.6530868620612, 1665.5400150564965, 920.1640527624404, 578.1281116316095, 757.7248539539287, 623.0386264882982, 896.5259130013874, 324.33839874586556, 1073.5240579987876, 138.0101018253481, 2342.80872209114, 3740.9466010888573, 527.8658012090018, 1266.8112860368565, 4185.888718520873, 1371.820583469118, 432.65652610512916, 2768.0412299021846, 1227.0844220545841, 1104.124662005459, 121.40531373966951, 2786.7994379891315, 2305.9134778075386, 595.5651503590634, 797.4656832867768, 517.6538139964687, 712.8353422244545, 507.8783585423371, 2326.134222074761, 994.9778653462417, 266.5684022931382, 1323.6204877537675, 1078.2568489098921, 118.8035205610795, 1252.1354526294162, 595.8914294436108, 1068.7300568192732, 500.5014879700029, 1595.9493135483935, 62.217691599042155, 173.93412707082462, 1090.614061593311, 826.96772580978, 476.9695420777425, 301.4808114786865, 2038.1935155385872, 252.29312496713828, 667.468056845828, 530.9193604049506, 788.022928974242, 1851.5194293633103, 333.6230206254404, 445.7115161843831, 63.92722413537558, 937.9357078898465, 709.0655713943997, 1639.4270182503387, 994.7675732176285, 670.6779863480479, 415.4180429955013, 1261.4054009441752, 403.50797890801914, 2487.2765772909625, 472.19053155428264, 491.9080760086654, 2873.6707448578672, 36.35397382522933, 885.5609568214277, 650.8324645218672, 1563.8917228112696, 871.8873322204454, 33.323605582816526, 373.88813977187965, 332.1646512930747, 320.5986535261618, 1193.8553812797181, 203.0575105973985, 150.29248021612875, 51.75333132711239, 828.4654938152526, 946.6009470012505, 873.4095504852012, 271.0061979102902, 629.125117512187, 1996.2603525525192, 1026.1440187578555, 2862.2802373636514, 184.4440043073846, 1728.755174492835, 665.806362663745, 340.85193024319597, 189.9842732596444, 420.1567537323572, 880.022120156209, 4939.664965744596, 1217.0216335882433, 1006.2503591942368, 167.6627846552292, 962.9653233571444, 1102.4442574573914, 91.26451553590596, 3262.8909485623008, 159.26840362383518, 328.001760232728, 838.4564007037552, 72.60753818647936, 3.1933496091514826, 640.8090186854824, 24.026325806044042, 178.38400888687465, 294.15004436438903, 36.34404245589394, 41.31452004495077, 577.1470279658679, 3306.524124455871, 756.580800543772, 358.52504670259077, 1266.7238791626878, 728.8688666104572, 0.9772565860766917, 194.72153572482057, 530.1818058434874, 388.0105669328477, 42.883175709983334, 133.10958622163162, 125.8103281109361, 620.1673464953201, 709.4028560870793, 1213.666112302104, 189.62396484194323, 300.22826299490407, 19.52072952548042, 70.90636159828864, 1629.090987838339, 617.6732402120251, 171.04601361858658, 595.7990180719644, 145.80657284823246, 10.126813846989535, 90.19300358684268, 141.38993479008786, 414.93422848405316, 126.830102601205, 291.027076850296, 1080.6704708763864, 39.76065976044629, 258.1672358646756, 188.80689857713878, 1139.2768685105257, 54.653613796806894, 716.2420761405956, 162.3719272983726, 20.837854865007102, 462.4779136882862, 738.6229448858649, 119.08791276230477, 237.33895239792764, 1284.5683389855549, 698.6040078937076, 128.13406173419207, 73.61808659264352, 241.47334488469642, 891.9193803686649, 1354.8170011993498, 86.62187150772661, 433.9434947153786, 382.0492149865022, 360.5387502515223, 1512.3708229649346, 767.7477004327811, 221.22479065158404, 194.8286008015275, 1711.4943518885411, 406.3397326346021, 79.22586347220931, 172.5797217268264, 885.8477138518356, 299.14356060128193, 265.0076512361411, 615.9449955101591, 227.12820273940451, 451.7577697833767, 252.8175453209551, 1421.2615893763723, 286.0167204536265, 2.264047327800654, 366.99380581232253, 1311.2198312303517, 609.5454475474544, 842.4113501717802, 488.84423793130554, 457.7583936128067, 247.38532120978925, 214.61571519996505, 366.87473661638796, 696.2737543864641, 5.794867799966596, 117.05136033508461, 464.8980381679721, 312.13000219897367, 369.15589595097117, 125.86114805738907, 270.2032876368612, 320.9028438186506, 200.3314302074723, 887.2801950178109, 850.7352691786364, 14.69062750437297, 404.05046425480396, 1712.3662381037138, 399.0584956886014, 342.6114053671481, 829.1263021748746, 379.127216158784, 2288.641382406815, 1232.1584859952563, 341.0305142381694, 1089.0954917548224, 71.19059710053261, 338.5694799195044, 61.64975862915162, 379.99465838377364, 611.0993875244167, 619.7383271086728, 718.162156757433, 398.7568090459099, 276.7958025743719, 2208.1416385759367, 280.9053443688899, 581.2675736809615, 1050.523799286806, 83.67444877687376, 814.335318531259, 587.5086532779969, 394.8239024800714, 21.805203955154866, 731.9400809870567, 327.21531951986253, 128.673174260417, 642.551812996855, 297.55511624144856, 169.28378577635158, 613.9193472976331, 543.8932529877638, 417.88572574185673, 61.03836513857823, 661.7257887759479, 299.326599475462, 59.145217229030095, 2088.1129678197904, 40.876307649654336, 2515.240571871982, 176.41811204352416, 1123.123143319739, 21.809568733209744, 230.75415287911892, 338.25243139686063, 990.40651792055, 793.1578855791595, 246.1585105265258, 186.19753769261297, 1665.6162601988763, 1273.9005143812392, 970.9242729910184, 1503.19159317296, 1012.3395919484319, 580.4949493235908, 137.02462734212168, 131.8321834888775, 1560.056395370164, 401.53378183045425, 279.3621538778534, 304.6918467503274, 230.29028788977303, 324.9027089229785, 212.49043805303518, 367.4338215511525, 220.32279236370232, 84.00074181205127, 417.8053909983719, 11.617784752161242, 104.38451625441667, 128.96782258024905, 26.57457660487853, 231.03556589898653, 329.920061433455, 40.22284566133749, 159.61627853510436, 176.20202320371754, 59.44799861032516, 125.96878592274152, 985.3562134181848, 579.776880910038, 485.5298052529106, 344.84561838035006, 79.14124817994889, 61.377032145042904, 167.00009427405894, 542.0697338738246, 71.23551299329847, 458.39068338635843, 280.521586209652, 114.6627587453695, 1416.3351574357366, 284.1836448289687, 402.84788269083947, 235.0719019524986, 40.60421613056678, 472.6002174965106, 60.11535905848723, 268.531931364676, 59.617083096527494, 93.68044126126915, 153.31872623693198, 158.06009123567492, 121.34565657773055, 42.59226538846269, 8.337241356493905, 901.9259564545937, 36.46452356420923, 131.7373174407985, 45.25165039545391, 10.86486204341054, 493.7930491361767, 802.3265969532076, 164.64397683204152, 51.36897966521792, 802.0529248800594, 148.12993210856803, 275.07179847406223, 5.226869923644699, 321.35708328627516, 989.9724032732192, 93.01049515767954, 396.51839028391987, 3.7564880534773692, 388.4481859328225, 79.90109641267918, 276.5895432771649, 139.37442705058493, 146.58999457012396, 168.28407741361298, 64.02543799264822, 1866.0158339139307, 761.6233606657479, 176.4160726100672, 80.49509758676868, 207.312598251272, 180.64520514197648, 53.183904221630655, 41.66758801310789, 221.2946882987162, 212.36084284470417, 10.995175683870912, 87.67020363849588, 153.6568526586052, 26.73785017535556, 17.352218199404888, 106.04454638890456, 16.25976818043273, 154.6953807452228, 195.7485451158136, 80.98433326580562, 474.2774183251895, 1016.8569448243361, 37.024647121783346, 62.95732955716085, 35.56376285420265, 107.13097201334313, 1067.7438057776308, 1319.8791878492339, 292.8029943180736, 1566.7231629471062, 173.07086846849415, 412.7067354742903, 356.83074993197806, 131.4015581246931, 3150.21665795031, 791.3675373113947, 405.1246247155359, 22.079640284413472, 1494.900623191148, 115.45963441359345, 338.9039248175686, 581.9274774022633, 355.99313075782266, 3706.0494050879497, 9.459206038387492, 161.93905472313054, 1517.8763015265577, 1587.511495175073, 862.0254842140712, 1001.8637961777858, 794.0719012304908, 86.52210541197564, 339.34866808506195, 1288.7117278879741, 523.5396450196858, 165.2721491161501, 125.08969595481176, 1485.0247436177451, 1168.254290466779, 245.13729807524942, 210.85766025877092, 2372.1040197268594, 187.27664050424937, 523.3629763011122, 920.9884229088202, 785.809013220598, 461.8261733663967, 818.7512114490382, 475.2130484995432, 3129.44292334246, 62.813203935162164, 437.29156796389725, 404.02770947362296, 520.7521821180126, 1383.17895243573, 482.98992016655393, 1160.5527687552385, 1380.7784600404557, 1417.9585846066475, 519.6950594371883, 2843.80121437856, 96.23811363428831, 543.6904078306397, 101.82798237225506, 89.53881027910393, 531.8700724869268, 893.8435639264062, 260.11573627009057, 381.2450302549405, 465.30448471335694, 566.2603472326882, 2239.8612023950554, 134.79597328940872, 418.63861307001207, 164.89990463061258, 2337.226168124238, 217.31694912083913, 628.2519292373909, 474.1522437245585, 435.8242099705385, 29.76935920852702, 11.85265226662159, 30.690071867313236, 34.74262966657989, 1017.3815841752803, 374.95161284017377, 328.06649625033606, 571.4322486934252, 1408.3931029862724, 102.81537070102058, 706.7952815640019, 678.1510392258642, 92.47201137035154, 380.1582057415508, 312.06454416387714, 1041.616836668225, 897.361039850628, 192.01803018455394, 235.63530831644312, 341.97235601139255, 328.156362606911, 323.4377265397925, 168.26275640772656, 203.19858308532275, 232.76124836876988, 685.5018801904516, 202.36341829260346, 315.6316762082279, 265.446639079717, 400.07386506104376, 728.866089547635, 65.225492260186, 62.844118611421436, 142.22709121287335, 13.045462457695976, 120.01348600583151, 688.5019591770833, 882.7684683079133, 798.8072171111125, 2.1439284165389836, 362.37512412294745, 3936.6975113149965, 1250.416369558312, 622.9481967417523, 816.74166310078, 3158.9938966224436, 2681.0358220043126, 45.02622836653609, 364.857647295692, 44.34322584036272, 1142.9697285146685, 602.2815788420849, 1850.9749775597593, 319.91317244712263, 2293.696089395089, 552.7744474824285, 566.1572558307089, 59.6728796325624, 367.714725054102, 734.1862188472878, 201.86250545224175, 134.0249904675875, 818.8881266918033, 268.437617545831, 66.13509828329552, 576.4663849162171, 533.4794707226101, 261.4213000914315, 289.63764121674467, 8.979756207787432, 110.39244608441368, 1453.8037818120793, 663.0019148138817, 179.48954837128986, 106.13464081392158, 2707.722363110399, 1004.3380320159486, 1055.2847104748944, 814.5432749383617, 94.76031024358235, 1424.766561895609, 1245.436804932775, 0.11607829900458455, 123.59866923838854, 537.939022347331, 677.8570872730343, 547.262250839849, 633.9218740027864, 340.0926072958391, 536.1879429141991, 1351.091415001545, 554.3622272559442, 2507.41084757098, 1999.212610301096, 404.53873501962516, 411.7234376472188, 60.66295394068584, 52.925325321615674, 925.8453455753624, 113.060888017877, 1166.283442875836, 500.46676850749645, 1498.2975336166564, 986.8267967872089, 1073.49846455839, 2121.164016814786, 985.2452967759455, 338.34260557126254, 425.19526453479193, 153.01153356337454, 3475.3661394214723, 727.0382242412306, 2201.487038129242, 2742.1835066744825, 569.1765096116578, 1078.4108148619998, 1644.8506173095666, 566.8525741640478, 2726.105661431444, 990.218281190726, 3150.5032330153044, 687.0669298515422, 4242.614038654603, 255.8508081055479, 4421.481423558318, 619.1733880236279, 7694.372498979908, 205.48551378620323, 467.4463524763705, 464.95601118553896, 975.2509211235447, 1467.751919926959, 107.60646617237944, 871.6901212019147, 1256.520727291354, 1806.200162361958, 5698.299217238557, 1872.9116855169414, 1301.1926552497316, 2670.5321338985814, 57.33901106007397, 51.60812607174739, 1491.6628859417979, 1894.5999387183692]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4514.03572076],\n",
       "        [ 462.77807862],\n",
       "        [1020.9455673 ],\n",
       "        ...,\n",
       "        [1809.46620478],\n",
       "        [1501.16834833],\n",
       "        [1188.46171432]],\n",
       "\n",
       "       [[ 462.77807862],\n",
       "        [1020.9455673 ],\n",
       "        [1070.35415506],\n",
       "        ...,\n",
       "        [1501.16834833],\n",
       "        [1188.46171432],\n",
       "        [ 646.00417853]],\n",
       "\n",
       "       [[1020.9455673 ],\n",
       "        [1070.35415506],\n",
       "        [5533.54376501],\n",
       "        ...,\n",
       "        [1188.46171432],\n",
       "        [ 646.00417853],\n",
       "        [ 190.71310761]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3936.69751131],\n",
       "        [1250.41636956],\n",
       "        [ 622.94819674],\n",
       "        ...,\n",
       "        [1301.19265525],\n",
       "        [2670.5321339 ],\n",
       "        [  57.33901106]],\n",
       "\n",
       "       [[1250.41636956],\n",
       "        [ 622.94819674],\n",
       "        [ 816.7416631 ],\n",
       "        ...,\n",
       "        [2670.5321339 ],\n",
       "        [  57.33901106],\n",
       "        [  51.60812607]],\n",
       "\n",
       "       [[ 622.94819674],\n",
       "        [ 816.7416631 ],\n",
       "        [3158.99389662],\n",
       "        ...,\n",
       "        [  57.33901106],\n",
       "        [  51.60812607],\n",
       "        [1491.66288594]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
