# GANs for sequences modeling
Supervised by Prof. [Jonathan Terhorst](http://www-personal.umich.edu/~jonth/). Special thanks to [Tianci Liu](https://lsa.umich.edu/stats/people/masters-students-of-applied-statistics/class-of-2019/liutianc.html) and Prof. [Jeffrey Regier](https://regier.stat.lsa.umich.edu/)

This repository consists of work of writing an honor thesis. The topic of this thesis is 
Below is the details of progress:
-  We first try some toy examples by trainin a GANs to model a mixed exponential distribution.  <br />
    The sequence looks like: 100\*exp(1), 1/100\*exp(1), 100\*exp(1), 1/100\*exp(1) <br />
     <br />
    In the [3/17 notebook](https://github.com/Pengjp/gene_research/blob/master/3_17_Pre_explore.ipynb), I made a training dataset with 50000 rows and 1000 columns then fit into a vanilla GAN. After training, I made two plots to show the generated data. I separated odd and even columns and calculated even for each columns of the two sepatated data. So the length of x-axis of each plot is 500 (1000 / 2). We can observe that the generated data is getting close to the real distribution. However, this is not a good to way to subjectively. <br />
- Then I took a step back to train a GAN for modeling an exponential distribution. <br />
  The sequence consists of numbers sampling from an exponentail distribution with rate 1. Then I scale the sequence with 100. This is sequence is one of the peak in the mixed exponential distribution I tried to model before. The result is in [3/30 notebook ](https://github.com/Pengjp/gene_research/blob/master/3_30_single_expo_model.ipynb). <br />  
  The first plot is a density plot showing the target distribution after transformation. The reason to do transformation is that deep learning perfers the input in a small range like we always do rescaling for image data. For this task, I added few layers to the original GAN and lower the learning rate for both generator and discirminator by referencing many tips of training GANs. In the bottom is checkpoints plots for every 200 epochs. The red curve is the true distribtuion and the blue curve is the generated distribution. We can observe that after about 1800 epochs, this GANs can give us a not-bad simulated data. THe best result is around 2000 epochs. However, after 2800 epochs, this GANs is not getting better. This is a common scenario as vanilla GAN is very unstable during training. It can be observed that the performance is getting back to it at 2000 epochs at the end of training iterations. Therefore, we could set stop points and save the model during the training iterations since longer training does not give better result.   <br />
  Another interesting observation is the accuracy of discriminator. It seems that accuracies of discriminating real is real and fake is fake do not associate with the performance. Additionally, the fake accuracy is zeor at first 1200 epochs and real accuracy is always one in the whole training process. These facts could mean some bugs in this GANs.  
